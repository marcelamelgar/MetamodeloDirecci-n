{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e521ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score, roc_curve, r2_score,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7741be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- config ----------------------------\n",
    "with open(\"config.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "EXCEL_PATH = cfg[\"excel_path\"]\n",
    "REAL_SHEET = cfg[\"real_sheet\"]\n",
    "PRED_SHEET = cfg[\"pred_sheet\"]\n",
    "OUTDIR = cfg[\"outdir\"]\n",
    "DEFAULT_WEEKS  = cfg[\"weeks\"]\n",
    "DEFAULT_TOLERANCE = cfg[\"tolerance\"]\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def savefig(path: str):\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=160, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def log(msg: str):\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] {msg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e107f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- carga ----------------------------\n",
    "def load_excel_fixed(path: str, real_sheet: str, pred_sheet: str):\n",
    "    real = pd.read_excel(path, sheet_name=real_sheet)\n",
    "    pred = pd.read_excel(path, sheet_name=pred_sheet)\n",
    "    # normalizacion preventiva\n",
    "    real.columns = [c.strip().lower().replace(\" \", \"_\") for c in real.columns]\n",
    "    pred.columns = [c.strip().lower().replace(\" \", \"_\") for c in pred.columns]\n",
    "    # parse fechas\n",
    "    real['date'] = pd.to_datetime(real['date'], errors=\"coerce\")\n",
    "    pred['date_requested']  = pd.to_datetime(pred['date_requested'], errors=\"coerce\")\n",
    "    pred['date_prediction'] = pd.to_datetime(pred['date_prediction'], errors=\"coerce\")\n",
    "    return real, pred\n",
    "\n",
    "def check_required_columns(real: pd.DataFrame, pred: pd.DataFrame):\n",
    "    # variables indispensables de cada dataframe\n",
    "    must_real = ['id_commodity','date','value']\n",
    "    must_pred = ['id_commodity','model','date_requested','date_prediction','prediction']\n",
    "    miss_real = [c for c in must_real if c not in real.columns]\n",
    "    miss_pred = [c for c in must_pred if c not in pred.columns]\n",
    "    # verificación que esten todas las columnas necesarias\n",
    "    if miss_real or miss_pred:\n",
    "        print(\"dataset insuficiente: faltan columnas obligatorias.\")\n",
    "        print(\"faltan en real:\", miss_real)\n",
    "        print(\"faltan en predicted:\", miss_pred)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6509f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- eda básica ----------------------------\n",
    "def dump_df_info(df: pd.DataFrame, name: str, outdir: str):\n",
    "    # guarda head/info/describe y chequeos simples de un dataframe\n",
    "    df.head(20).to_csv(os.path.join(outdir, f\"{name}_head.csv\"), index=False)\n",
    "    df.describe(include='all').to_csv(os.path.join(outdir, f\"{name}_describe.csv\"))\n",
    "    buf = StringIO(); df.info(buf=buf)\n",
    "    with open(os.path.join(outdir, f\"{name}_info.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(buf.getvalue())\n",
    "    df.isna().sum().to_csv(os.path.join(outdir, f\"{name}_nulls.csv\"))\n",
    "    with open(os.path.join(outdir, f\"{name}_dups.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"duplicados_exactos: {int(df.duplicated().sum())}\\n\")\n",
    "\n",
    "def basic_counts(real: pd.DataFrame, pred: pd.DataFrame, outdir: str):\n",
    "    # commodities por tipo (si existe)\n",
    "    if 'type' in real.columns and 'id_commodity' in real.columns:\n",
    "        real.groupby('type')['id_commodity'].nunique().sort_values(ascending=False)\\\n",
    "            .to_csv(os.path.join(outdir, \"count_commodities_by_type.csv\"))\n",
    "    # modelos disponibles\n",
    "    if 'model' in pred.columns:\n",
    "        pred['model'].value_counts().to_csv(os.path.join(outdir, \"count_models.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a518e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- construcción del dataset ----------------------------\n",
    "def filter_horizon(pred: pd.DataFrame, weeks: int, tolerance: int):\n",
    "    pred = pred.copy()\n",
    "    pred['horizon_days'] = (pred['date_prediction'] - pred['date_requested']).dt.days\n",
    "    lo = weeks*7 - tolerance\n",
    "    hi = weeks*7 + tolerance\n",
    "    # 4 semanas - diferencia numerica para identificar subidas o bajadas\n",
    "    pred4 = pred[(pred['horizon_days'] >= lo) & (pred['horizon_days'] <= hi)].copy()\n",
    "    return pred4, lo, hi\n",
    "\n",
    "def prepare_real_agg(real: pd.DataFrame):\n",
    "    agg = {'value': 'mean'}\n",
    "    # verificacion de columnas categoricas\n",
    "    for cat in ['type','incoterm','origin','publication']:\n",
    "        if cat in real.columns:\n",
    "            # si hay duplicados, se toma el primero\n",
    "            agg[cat] = 'first'\n",
    "    real_agg = real.groupby(['id_commodity','date'], as_index=False).agg(agg)\n",
    "    return real_agg\n",
    "\n",
    "def merge_with_real(pred4: pd.DataFrame, real_agg: pd.DataFrame):\n",
    "    # left join - se mantienen todas las predicciones\n",
    "    merged = pred4.merge(\n",
    "        real_agg,\n",
    "        left_on=['id_commodity','date_prediction'],\n",
    "        right_on=['id_commodity','date'],\n",
    "        how='left',\n",
    "        suffixes=('','_real')\n",
    "    )\n",
    "    return merged\n",
    "\n",
    "def add_target_and_errors(merged: pd.DataFrame):\n",
    "    merged = merged.copy()\n",
    "    # 1 si valor real es > a la prediccion, 0 si el valor fue menor o igual\n",
    "    merged['direccion']    = (merged['value'] > merged['prediction']).astype(int)\n",
    "    # + predijo más bajo que la realidad, - predijo más alto que la realidad\n",
    "    merged['error_signed'] = merged['value'] - merged['prediction']\n",
    "    # cuánto se equivocó el modelo sin importar la dirección\n",
    "    merged['error_abs']    = (merged['value'] - merged['prediction']).abs()\n",
    "    return merged\n",
    "\n",
    "def clean_critical(merged: pd.DataFrame):\n",
    "    crit = ['id_commodity','model','date_prediction','prediction','value','direccion']\n",
    "    merged = merged.loc[merged[crit].notna().all(axis=1)].copy()\n",
    "    return merged\n",
    "\n",
    "def build_wide_and_consensus(merged: pd.DataFrame):\n",
    "    # un commodity por fecha de prediccion\n",
    "    key_cols = ['id_commodity','date_prediction']\n",
    "    wide = merged.pivot_table(index=key_cols, columns='model', values='prediction', aggfunc='mean')\n",
    "    # renombre para mejor entendimiento\n",
    "    wide.columns = [f\"pred_{str(c)}\" for c in wide.columns]\n",
    "    wide = wide.reset_index()\n",
    "    # features de consenso\n",
    "    pred_cols = [c for c in wide.columns if c.startswith(\"pred_\")]\n",
    "    wide['consensus_mean']  = wide[pred_cols].mean(axis=1, skipna=True)\n",
    "    wide['consensus_std']   = wide[pred_cols].std(axis=1, ddof=0, skipna=True)\n",
    "    wide['consensus_min']   = wide[pred_cols].min(axis=1, skipna=True)\n",
    "    wide['consensus_max']   = wide[pred_cols].max(axis=1, skipna=True)\n",
    "    wide['consensus_range'] = wide['consensus_max'] - wide['consensus_min']\n",
    "    return wide\n",
    "\n",
    "def attach_targets_and_cats(merged: pd.DataFrame, wide: pd.DataFrame):\n",
    "    key_cols = ['id_commodity','date_prediction']\n",
    "    cat_cols = [c for c in ['type','incoterm','origin','publication'] if c in merged.columns]\n",
    "    # dirección - etiqueta binaria, value - valor real del precio, horizon_days - diferencia entre fechas\n",
    "    targets = merged.groupby(key_cols, as_index=False).agg({'direccion':'first','value':'first','horizon_days':'first'})\n",
    "    for c in cat_cols:\n",
    "        targets = targets.merge(merged[key_cols+[c]].drop_duplicates(), on=key_cols, how='left')\n",
    "    # features de predicción (wide + consenso) con el dataset de targets + categorías\n",
    "    features0 = wide.merge(targets, on=key_cols, how='inner')\n",
    "    return features0\n",
    "\n",
    "def add_hist_errors_train_only(merged: pd.DataFrame, features0: pd.DataFrame):\n",
    "    # percentil 70%\n",
    "    # todo lo anterior a esa fecha = train, despues = test\n",
    "    date_cut = features0['date_prediction'].quantile(0.7)\n",
    "    train_idx = features0['date_prediction'] <= date_cut\n",
    "    # se queda solo con filas de entrenamiento\n",
    "    key_cols = ['id_commodity','date_prediction']\n",
    "    train_keys = features0.loc[train_idx, key_cols]\n",
    "    train_long = merged.merge(train_keys, on=key_cols, how='inner')\n",
    "    hist = train_long.groupby(['id_commodity','model'], as_index=False).agg(\n",
    "        # hist_signed_mean - si subestima o sobreestima, hist_abs_mean - precisión, hist_count - cantidad de observaciones\n",
    "        hist_signed_mean=('error_signed','mean'),\n",
    "        hist_abs_mean=('error_abs','mean'),\n",
    "        hist_count=('error_abs','count')\n",
    "    )\n",
    "    # cada fila un id_commodity, cada columna las metricas historicas para cada modelo\n",
    "    hw = hist.pivot_table(index='id_commodity', columns='model',\n",
    "                          values=['hist_signed_mean','hist_abs_mean','hist_count'], aggfunc='first')\n",
    "    hw.columns = [f\"{a}_{b}\" for a,b in hw.columns]\n",
    "    hw = hw.reset_index()\n",
    "    return features0.merge(hw, on='id_commodity', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea436b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- r² en test ----------------------------\n",
    "def r2_on_test(features: pd.DataFrame, wide: pd.DataFrame, test_idx: pd.Series, outdir: str):\n",
    "    # calcula r² de predicciones numéricas vs 'value' en test; guarda csv por modelo\n",
    "    out = {}\n",
    "    key_cols = ['id_commodity','date_prediction']\n",
    "    # consenso\n",
    "    if {'consensus_mean','value'}.issubset(features.columns):\n",
    "        dfc = features.loc[test_idx, key_cols + ['consensus_mean','value']].dropna()\n",
    "        if not dfc.empty:\n",
    "            out['r2_consensus_mean'] = float(r2_score(dfc['value'], dfc['consensus_mean']))\n",
    "    # por modelo\n",
    "    pred_cols = [c for c in wide.columns if c.startswith('pred_')]\n",
    "    if pred_cols:\n",
    "        test_keys = features.loc[test_idx, key_cols].drop_duplicates()\n",
    "        wtest = test_keys.merge(wide[key_cols + pred_cols], on=key_cols, how='left')\\\n",
    "                         .merge(features[key_cols + ['value']], on=key_cols, how='left')\n",
    "        rows = []\n",
    "        for c in pred_cols:\n",
    "            dfm = wtest[['value', c]].dropna()\n",
    "            if not dfm.empty:\n",
    "                r2c = float(r2_score(dfm['value'], dfm[c]))\n",
    "                out[f\"r2_{c}\"] = r2c\n",
    "                rows.append({'model': c, 'r2': r2c})\n",
    "        if rows:\n",
    "            pd.DataFrame(rows).sort_values('r2', ascending=False)\\\n",
    "              .to_csv(os.path.join(outdir, \"r2_by_model_test.csv\"), index=False)\n",
    "    return out\n",
    "\n",
    "# ---------------------------- modelado y evaluación ----------------------------\n",
    "def build_pipeline(X: pd.DataFrame):\n",
    "    categorical_cols = [c for c in ['type','incoterm','origin','publication'] if c in X.columns]\n",
    "    numeric_cols = [c for c in X.columns if c not in categorical_cols]\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            # rellena valores faltantes con la mediana\n",
    "            (\"num\", SimpleImputer(strategy=\"median\"), numeric_cols),\n",
    "            # reemplaza nulos por categoría más frecuente y hace one-hot\n",
    "            (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                              (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))]), categorical_cols),\n",
    "        ],\n",
    "        # ignora columnas que no se hayan listado\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    # deja que los árboles crezcan hasta que se cumpla otra condición\n",
    "    # mínimo 4 muestras para dividir un nodo\n",
    "    # mínimo 2 muestras en cada hoja\n",
    "    # hace reproducible el resultado\n",
    "    # le da más peso a la minoritaria\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=400, max_depth=None, min_samples_split=4, min_samples_leaf=2,\n",
    "        random_state=RANDOM_SEED, n_jobs=-1, class_weight=\"balanced\"\n",
    "    )\n",
    "    clf = Pipeline([(\"prep\", preprocess), (\"rf\", rf)])\n",
    "    return clf, numeric_cols, categorical_cols\n",
    "\n",
    "def compute_baselines(wide: pd.DataFrame, merged: pd.DataFrame):\n",
    "    # líneas base de comparación\n",
    "    out = {}\n",
    "    key_cols = ['id_commodity','date_prediction']\n",
    "    # dataset reducido con un valor real y la dirección (0/1) para cada predicción\n",
    "    base = merged.groupby(key_cols, as_index=False).agg({'value':'first','direccion':'first'})\n",
    "    # clase mayoritaria\n",
    "    maj = int(base['direccion'].value_counts().idxmax())\n",
    "    out['majority_class'] = float((base['direccion']==maj).mean())\n",
    "    # mejor modelo individual\n",
    "    pred_cols = [c for c in wide.columns if c.startswith(\"pred_\")]\n",
    "    best_acc, best_model = None, None\n",
    "    tmp = base.merge(wide[key_cols+pred_cols], on=key_cols, how='left')\n",
    "    for c in pred_cols:\n",
    "        pred_dir = (tmp['value'] > tmp[c]).astype(int)\n",
    "        acc = float((pred_dir == tmp['direccion']).mean())\n",
    "        if best_acc is None or acc > best_acc:\n",
    "            best_acc, best_model = acc, c\n",
    "    out['best_single_model_acc'] = best_acc\n",
    "    out['best_single_model_name'] = best_model\n",
    "    return out\n",
    "\n",
    "def evaluate_and_save(clf, X_test, y_test, outdir, y_proba=None, tag=\"TEST\"):\n",
    "    # medir rendimiento en test\n",
    "    y_pred = clf.predict(X_test)\n",
    "    metrics = {\n",
    "        # proporción de predicciones correctas\n",
    "        \"accuracy\":  float(accuracy_score(y_test, y_pred)),\n",
    "        # de los predichos como 1 (subida), cuántos eran realmente 1\n",
    "        \"precision\": float(precision_score(y_test, y_pred, zero_division=0)),\n",
    "        # de los que eran realmente 1, cuántos detectó el modelo\n",
    "        \"recall\":    float(recall_score(y_test, y_pred, zero_division=0)),\n",
    "        # media entre precision y recall (balance)\n",
    "        \"f1\":        float(f1_score(y_test, y_pred, zero_division=0)),\n",
    "    }\n",
    "    if y_proba is not None and len(np.unique(y_test))>1:\n",
    "        # capacidad de ordenar bien positivos vs negativos\n",
    "        metrics[\"roc_auc\"] = float(roc_auc_score(y_test, y_proba))\n",
    "    else:\n",
    "        metrics[\"roc_auc\"] = None\n",
    "    return metrics, y_pred\n",
    "\n",
    "def threshold_tuning(y_true, y_proba, outdir, tag=\"TEST\"):\n",
    "    if y_proba is None:\n",
    "        return None\n",
    "    ths = np.linspace(0.05, 0.95, 19)\n",
    "    best = (None, None)\n",
    "    pts = []\n",
    "    # convierte probabilidades a etiquetas binarias y calcula F1\n",
    "    for t in ths:\n",
    "        y_hat = (y_proba >= t).astype(int)\n",
    "        f1 = f1_score(y_true, y_hat, zero_division=0)\n",
    "        pts.append((float(t), float(f1)))\n",
    "        if best[1] is None or f1 > best[1]:\n",
    "            best = (float(t), float(f1))\n",
    "    df = pd.DataFrame(pts, columns=[\"threshold\",\"f1\"])\n",
    "    # gráfica\n",
    "    plt.figure()\n",
    "    plt.plot(df[\"threshold\"], df[\"f1\"], marker=\"o\")\n",
    "    plt.xlabel(\"Umbral\"); plt.ylabel(\"F1\"); plt.title(\"Tuning de umbral (F1)\")\n",
    "    savefig(os.path.join(outdir, f\"threshold_f1_{tag.lower()}.png\"))\n",
    "    return {\"best_threshold\": best[0], \"best_f1\": best[1]}\n",
    "\n",
    "def write_report(outdir, context):\n",
    "    # reporte sencillo en markdown\n",
    "    md = []\n",
    "    md.append(f\"# Informe — Metamodelo de dirección ({context['weeks']} semanas)\")\n",
    "    md.append(\"\")\n",
    "    md.append(f\"**Archivo**: `{context['excel_path']}`\")\n",
    "    md.append(f\"**Hojas**: Real=`{context['real_sheet']}`, Predicted=`{context['pred_sheet']}`\")\n",
    "    md.append(f\"**Ventana**: {context['lo_days']}–{context['hi_days']} días\")\n",
    "    md.append(f\"**Corte temporal 70/30**: {context['date_cut']}\")\n",
    "    md.append(\"\")\n",
    "    md.append(\"## Baselines\")\n",
    "    mc = context.get('baselines', {})\n",
    "    maj = mc.get('majority_class')\n",
    "    if maj is not None:\n",
    "        md.append(f\"- Clase mayoritaria (acc): **{maj:.4f}**\")\n",
    "    best_single = mc.get('best_single_model_name')\n",
    "    best_single_acc = mc.get('best_single_model_acc')\n",
    "    if best_single is not None and best_single_acc is not None:\n",
    "        md.append(f\"- Mejor modelo individual `{best_single}` (acc): **{best_single_acc:.4f}**\")\n",
    "\n",
    "    md.append(\"\")\n",
    "    md.append(\"## Métricas del metamodelo (TEST)\")\n",
    "    for k, v in context.get('metrics', {}).items():\n",
    "        if v is None:\n",
    "            md.append(f\"- {k}: NA\")\n",
    "        elif isinstance(v, (int, float, np.number)):\n",
    "            md.append(f\"- {k}: **{float(v):.4f}**\")\n",
    "        else:\n",
    "            md.append(f\"- {k}: **{v}**\")\n",
    "\n",
    "    # r² destacados (si existen)\n",
    "    if context['metrics'].get('r2_consensus') is not None:\n",
    "        md.append(f\"- r² (consenso): **{context['metrics']['r2_consensus']:.4f}**\")\n",
    "    if context['metrics'].get('best_r2_model') is not None and context['metrics'].get('best_r2_value') is not None:\n",
    "        md.append(f\"- mejor r² individual: `{context['metrics']['best_r2_model']}` = **{context['metrics']['best_r2_value']:.4f}**\")\n",
    "\n",
    "    if context.get(\"threshold\"):\n",
    "        md.append(\"\")\n",
    "        md.append(f\"**Umbral óptimo (F1)**: {context['threshold']['best_threshold']:.2f} → F1={context['threshold']['best_f1']:.4f}\")\n",
    "\n",
    "    md.append(\"\")\n",
    "    md.append(\"## Artefactos\")\n",
    "    md.append(\"- `metrics_summary.json`\")\n",
    "    md.append(\"- `feature_importances.csv` y `r2_by_model_test.csv`\")\n",
    "    md.append(\"- `predicciones_test.csv`\")\n",
    "    md.append(\"- Figuras: `dist_precios_reales.png`, `dist_horizon_days.png`, `feature_importances_top20.png`,\")\n",
    "    md.append(\"           `cm_test.png`, `roc_test.png` (si aplica), `pr_test.png`, `threshold_f1_test.png`,\")\n",
    "    md.append(\"           `threshold_metrics.png`, `scatter_consensus_vs_real_test.png`, `lift_curve.png`, `missing_by_model.png`\")\n",
    "\n",
    "    with open(os.path.join(outdir, \"reporte_metamodelo.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083dbdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- gráficas ----------------------------\n",
    "def plot_horizon_distribution(df, outdir, lo_days=None, hi_days=None):\n",
    "    # crea y guarda un histograma de los horizontes de predicción (en días)\n",
    "    plt.figure()\n",
    "    if 'horizon_days' not in df.columns:\n",
    "        if {'date_prediction','date_requested'}.issubset(df.columns):\n",
    "            df = df.copy()\n",
    "            df['horizon_days'] = (df['date_prediction'] - df['date_requested']).dt.days\n",
    "        else:\n",
    "            return\n",
    "    df['horizon_days'].plot(kind='hist', bins=40, title='Distribución de horizon_days')\n",
    "    if lo_days is not None and hi_days is not None:\n",
    "        ymin, ymax = plt.ylim()\n",
    "        plt.axvspan(lo_days, hi_days, alpha=0.2)\n",
    "        plt.text(lo_days, ymax*0.95, f\"{lo_days}-{hi_days}d\", va='top')\n",
    "    plt.xlabel('Días'); plt.ylabel('Frecuencia')\n",
    "    savefig(os.path.join(outdir, \"dist_horizon_days.png\"))\n",
    "\n",
    "def plot_feature_importances(clf, num_cols, cat_cols, outdir):\n",
    "    # qué variables usa más el random forest para decidir la dirección\n",
    "    importances = clf.named_steps['rf'].feature_importances_\n",
    "    ohe = clf.named_steps['prep'].named_transformers_.get('cat', None)\n",
    "    cat_names = ohe.named_steps['ohe'].get_feature_names_out(cat_cols).tolist() if cat_cols else []\n",
    "    feat_names = num_cols + cat_names\n",
    "    if len(importances) != len(feat_names):\n",
    "        feat_names = [f\"f{i}\" for i in range(len(importances))]\n",
    "    fi = pd.DataFrame({\"feature\": feat_names, \"importance\": importances}).sort_values(\"importance\", ascending=False).head(20)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.barh(fi[\"feature\"][::-1], fi[\"importance\"][::-1])\n",
    "    plt.title(\"Top 20 importancias\"); plt.xlabel(\"Importancia\")\n",
    "    savefig(os.path.join(outdir, \"feature_importances_top20.png\"))\n",
    "\n",
    "def plot_roc(y_test, y_proba, outdir):\n",
    "    # cada punto = desempeño del modelo en un umbral de probabilidad distinto\n",
    "    if y_proba is None or len(np.unique(y_test)) < 2: return\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC')\n",
    "    plt.plot([0,1],[0,1],'--')\n",
    "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('Curva ROC')\n",
    "    plt.legend()\n",
    "    savefig(os.path.join(outdir, \"roc_test.png\"))\n",
    "\n",
    "def plot_confusion(y_test, y_pred, outdir):\n",
    "    # diagonal principal (TN y TP) = aciertos; fuera de diagonal = errores\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title('Matriz de confusión'); plt.xlabel('Predicho'); plt.ylabel('Real')\n",
    "    for (i,j),v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha='center', va='center')\n",
    "    savefig(os.path.join(outdir, \"cm_test.png\"))\n",
    "\n",
    "def plot_pr_curve(y_test, y_proba, outdir):\n",
    "    # trade-off entre precision y recall a diferentes umbrales\n",
    "    if y_proba is None or len(np.unique(y_test)) < 2: return\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "    ap = average_precision_score(y_test, y_proba)\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label=f\"AP = {ap:.3f}\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Precision–Recall\")\n",
    "    plt.legend()\n",
    "    savefig(os.path.join(outdir, \"pr_test.png\"))\n",
    "\n",
    "def plot_threshold_sweep(y_test, y_proba, outdir):\n",
    "    # barrido de umbral para ver F1, precision y recall\n",
    "    if y_proba is None: return\n",
    "    ths = np.linspace(0.05, 0.95, 19)\n",
    "    rows = []\n",
    "    for t in ths:\n",
    "        y_hat = (y_proba >= t).astype(int)\n",
    "        rows.append({\n",
    "            \"threshold\": t,\n",
    "            \"f1\": f1_score(y_test, y_hat, zero_division=0),\n",
    "            \"precision\": precision_score(y_test, y_hat, zero_division=0),\n",
    "            \"recall\": recall_score(y_test, y_hat, zero_division=0)\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    plt.figure()\n",
    "    plt.plot(df[\"threshold\"], df[\"f1\"], marker=\"o\", label=\"F1\")\n",
    "    plt.plot(df[\"threshold\"], df[\"precision\"], marker=\"o\", label=\"Precision\")\n",
    "    plt.plot(df[\"threshold\"], df[\"recall\"], marker=\"o\", label=\"Recall\")\n",
    "    plt.xlabel(\"Umbral\"); plt.ylabel(\"Métrica\"); plt.title(\"Barrido de umbral\")\n",
    "    plt.legend()\n",
    "    savefig(os.path.join(outdir, \"threshold_metrics.png\"))\n",
    "\n",
    "def plot_timeseries_one_commodity(real, wide, commodity_id, outdir):\n",
    "    # evolución de precios reales, predicciones de modelos, consenso promedio\n",
    "    if 'id_commodity' not in real.columns or 'date' not in real.columns or 'value' not in real.columns:\n",
    "        return\n",
    "    df_real = real[real['id_commodity'] == commodity_id].sort_values('date')\n",
    "    if df_real.empty: return\n",
    "    plt.figure(figsize=(9,4))\n",
    "    plt.plot(df_real['date'], df_real['value'], label='Real')\n",
    "    if {'id_commodity','date_prediction'}.issubset(wide.columns):\n",
    "        w = wide[wide['id_commodity']==commodity_id].copy()\n",
    "        if not w.empty:\n",
    "            pred_cols = [c for c in w.columns if c.startswith('pred_')]\n",
    "            w = w.sort_values('date_prediction')\n",
    "            for c in pred_cols[:4]:\n",
    "                plt.plot(w['date_prediction'], w[c], label=c)\n",
    "            if 'consensus_mean' in w.columns:\n",
    "                plt.plot(w['date_prediction'], w['consensus_mean'], linewidth=2, label='consensus_mean')\n",
    "    plt.title(f\"Serie temporal — id_commodity={commodity_id}\")\n",
    "    plt.xlabel('Fecha'); plt.ylabel('Precio'); plt.legend()\n",
    "    savefig(os.path.join(outdir, f\"ts_{commodity_id}.png\"))\n",
    "\n",
    "def plot_scatter_consensus_vs_real(features, test_idx, outdir):\n",
    "    # si los puntos están alineados a la diagonal, el consenso predice bien los valores reales\n",
    "    need = {'consensus_mean','value'}\n",
    "    if not need.issubset(features.columns): return\n",
    "    df = features.loc[test_idx, ['consensus_mean','value']].dropna()\n",
    "    if df.empty: return\n",
    "    plt.figure()\n",
    "    plt.scatter(df['consensus_mean'], df['value'], s=14)\n",
    "    lims = [min(df.min())*0.95, max(df.max())*1.05]\n",
    "    plt.plot(lims, lims, '--')\n",
    "    plt.xlabel('Consenso (media predicciones)'); plt.ylabel('Valor real')\n",
    "    plt.title('Consenso vs Real (TEST)')\n",
    "    savefig(os.path.join(outdir, \"scatter_consensus_vs_real_test.png\"))\n",
    "\n",
    "def plot_lift_curve(y_test, y_proba, outdir):\n",
    "    # mientras más arriba esté la curva respecto a la línea de 1, más útil es el modelo\n",
    "    if y_proba is None: return\n",
    "    df = pd.DataFrame({\"y\": y_test.values, \"p\": y_proba}).sort_values(\"p\", ascending=False).reset_index(drop=True)\n",
    "    df[\"cum_pos\"] = df[\"y\"].cumsum()\n",
    "    total_pos = df[\"y\"].sum()\n",
    "    perc = np.arange(1, len(df)+1)/len(df)\n",
    "    lift = df[\"cum_pos\"] / (total_pos * perc + 1e-9)\n",
    "    plt.figure()\n",
    "    plt.plot(perc*100, lift)\n",
    "    plt.axhline(1.0, linestyle='--')\n",
    "    plt.xlabel('% de población (ordenada por prob.)'); plt.ylabel('Lift'); plt.title('Lift curve')\n",
    "    savefig(os.path.join(outdir, \"lift_curve.png\"))\n",
    "\n",
    "def plot_missing_by_model(wide, outdir):\n",
    "    # qué tan incompletas están las predicciones por modelo\n",
    "    pred_cols = [c for c in wide.columns if c.startswith(\"pred_\")]\n",
    "    if not pred_cols: return\n",
    "    miss = wide[pred_cols].isna().mean().sort_values(ascending=False)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.barh(miss.index[::-1], miss.values[::-1])\n",
    "    plt.xlabel('Proporción de NaN'); plt.title('Faltantes por modelo (pred_*)')\n",
    "    savefig(os.path.join(outdir, \"missing_by_model.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020128a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c85bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- main ----------------------------\n",
    "def main():\n",
    "\n",
    "    # asegurar que exista el directorio de salida\n",
    "    ensure_dir(OUTDIR)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "    # 1) carga de hojas excel (real y pred)\n",
    "    log(\"cargando excel…\")\n",
    "    real, pred = load_excel_fixed(EXCEL_PATH, REAL_SHEET, PRED_SHEET)\n",
    "\n",
    "    # inspección básica y conteos\n",
    "    dump_df_info(real, \"real\", OUTDIR)\n",
    "    dump_df_info(pred, \"predicted\", OUTDIR)\n",
    "    basic_counts(real, pred, OUTDIR)\n",
    "\n",
    "    # validación mínima de columnas obligatorias\n",
    "    if not check_required_columns(real, pred):\n",
    "        sys.exit(0)\n",
    "\n",
    "    # 2) filtrar predicciones al horizonte de 4 semanas\n",
    "    pred4, lo_days, hi_days = filter_horizon(pred, DEFAULT_WEEKS, DEFAULT_TOLERANCE)\n",
    "    if pred4.empty:\n",
    "        print(\"dataset insuficiente: sin cobertura para el horizonte seleccionado.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    # 3) merge con precios reales y creación de variables target y error\n",
    "    real_agg = prepare_real_agg(real)\n",
    "    merged = merge_with_real(pred4, real_agg)\n",
    "    merged = add_target_and_errors(merged)\n",
    "    merged = clean_critical(merged)\n",
    "    if merged.empty:\n",
    "        print(\"dataset insuficiente: después de limpiar críticos quedó vacío.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    # 4) crear dataset wide + consenso + targets + categorías + errores históricos\n",
    "    wide = build_wide_and_consensus(merged)\n",
    "    features0 = attach_targets_and_cats(merged, wide)\n",
    "    if features0.empty or 'date_prediction' not in features0.columns or 'direccion' not in features0.columns:\n",
    "        print(\"dataset insuficiente: revisa el contenido de las hojas.\")\n",
    "        sys.exit(0)\n",
    "    features = add_hist_errors_train_only(merged, features0)\n",
    "\n",
    "    # 5) dividir datos en train y test según corte temporal (70/30)\n",
    "    date_cut = features['date_prediction'].quantile(0.7)\n",
    "    train_idx = features['date_prediction'] <= date_cut\n",
    "    test_idx  = features['date_prediction'] > date_cut\n",
    "\n",
    "    # 6) calcular baselines (clase mayoritaria y mejor modelo individual)\n",
    "    baselines = compute_baselines(wide, merged)\n",
    "\n",
    "    # 7) entrenamiento del modelo random forest con pipeline de preprocesamiento\n",
    "    y = features['direccion'].astype(int)\n",
    "    X = features.drop(columns=['direccion','date_prediction','id_commodity'], errors='ignore')\n",
    "    clf, num_cols, cat_cols = build_pipeline(X)\n",
    "    X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
    "    X_test,  y_test  = X.loc[test_idx],  y.loc[test_idx]\n",
    "    if len(y_train)==0 or len(y_test)==0:\n",
    "        print(\"dataset insuficiente: split temporal dejó folds vacíos.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    log(\"entrenando modelo…\")\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # probabilidad de predicción para la clase positiva\n",
    "    try:\n",
    "        y_proba = clf.predict_proba(X_test)[:,1]\n",
    "    except Exception:\n",
    "        y_proba = None\n",
    "\n",
    "    # evaluación del modelo en test y ajuste de umbral\n",
    "    metrics, y_pred = evaluate_and_save(clf, X_test, y_test, OUTDIR, y_proba=y_proba, tag=\"TEST\")\n",
    "    tuning = threshold_tuning(y_test, y_proba, OUTDIR, tag=\"TEST\") if y_proba is not None else None\n",
    "\n",
    "    # r² en test (consenso y por modelo)\n",
    "    r2_dict = r2_on_test(features, wide, test_idx, OUTDIR)\n",
    "    if r2_dict:\n",
    "        if 'r2_consensus_mean' in r2_dict:\n",
    "            metrics['r2_consensus'] = r2_dict['r2_consensus_mean']\n",
    "        best_r2 = None; best_name = None\n",
    "        for k,v in r2_dict.items():\n",
    "            if k.startswith('r2_pred_'):\n",
    "                if best_r2 is None or v > best_r2:\n",
    "                    best_r2, best_name = v, k.replace('r2_','')\n",
    "        if best_r2 is not None:\n",
    "            metrics['best_r2_model'] = best_name\n",
    "            metrics['best_r2_value'] = best_r2\n",
    "\n",
    "    # exportar importancias de variables a csv\n",
    "    importances = clf.named_steps['rf'].feature_importances_\n",
    "    ohe = clf.named_steps['prep'].named_transformers_.get('cat', None)\n",
    "    if ohe is not None and hasattr(ohe, \"named_steps\") and \"ohe\" in ohe.named_steps:\n",
    "        cat_names = ohe.named_steps['ohe'].get_feature_names_out(cat_cols).tolist()\n",
    "    else:\n",
    "        cat_names = []\n",
    "    feat_names = num_cols + cat_names\n",
    "    if len(importances) != len(feat_names):\n",
    "        feat_names = [f\"f{i}\" for i in range(len(importances))]\n",
    "    fi = pd.DataFrame({\"feature\": feat_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
    "    fi.to_csv(os.path.join(OUTDIR, \"feature_importances.csv\"), index=False)\n",
    "\n",
    "    # eda mínima: distribución de valores reales\n",
    "    if 'value' in real.columns and np.isfinite(real['value']).any():\n",
    "        plt.figure()\n",
    "        real['value'].plot(kind='hist', bins=40, title='distribución de precios reales')\n",
    "        plt.xlabel('precio'); plt.ylabel('frecuencia')\n",
    "        savefig(os.path.join(OUTDIR, \"dist_precios_reales.png\"))\n",
    "\n",
    "    # exportar tabla de predicciones en test\n",
    "    key_cols = [c for c in ['id_commodity','date_prediction','type','incoterm','origin','publication'] if c in features.columns]\n",
    "    pred_table = features.loc[test_idx, key_cols].copy()\n",
    "    pred_table['y_true'] = y_test.values\n",
    "    pred_table['y_pred'] = (y_proba >= 0.5).astype(int) if y_proba is not None else y_pred\n",
    "    if y_proba is not None:\n",
    "        pred_table['proba_subida'] = y_proba\n",
    "    pred_cols = [c for c in wide.columns if c.startswith(\"pred_\")]\n",
    "    pred_table = pred_table.merge(wide[['id_commodity','date_prediction']+pred_cols], on=['id_commodity','date_prediction'], how='left')\n",
    "    pred_table.to_csv(os.path.join(OUTDIR, \"predicciones_test.csv\"), index=False)\n",
    "\n",
    "    # 8) gráficas principales\n",
    "    plot_horizon_distribution(pred4, OUTDIR, lo_days, hi_days)\n",
    "    plot_feature_importances(clf, num_cols, cat_cols, OUTDIR)\n",
    "    plot_roc(y_test, y_proba, OUTDIR)\n",
    "    plot_confusion(y_test, y_pred, OUTDIR)\n",
    "    plot_pr_curve(y_test, y_proba, OUTDIR)\n",
    "    plot_threshold_sweep(y_test, y_proba, OUTDIR)\n",
    "    if 'id_commodity' in real.columns and not real['id_commodity'].empty:\n",
    "        top_id = real['id_commodity'].value_counts().index[0]\n",
    "        plot_timeseries_one_commodity(real, wide, top_id, OUTDIR)\n",
    "    plot_scatter_consensus_vs_real(features, test_idx, OUTDIR)\n",
    "    plot_lift_curve(y_test, y_proba, OUTDIR)\n",
    "    plot_missing_by_model(wide, OUTDIR)\n",
    "\n",
    "    # 9) guardar resumen y reporte en json y markdown\n",
    "    summary = {\n",
    "        \"excel_path\": EXCEL_PATH,\n",
    "        \"real_sheet\": REAL_SHEET,\n",
    "        \"pred_sheet\": PRED_SHEET,\n",
    "        \"weeks\": DEFAULT_WEEKS,\n",
    "        \"lo_days\": lo_days, \"hi_days\": hi_days,\n",
    "        \"date_cut\": pd.to_datetime(date_cut).strftime(\"%Y-%m-%d\"),\n",
    "        \"n_train\": int(len(y_train)), \"n_test\": int(len(y_test)),\n",
    "        \"metrics\": metrics,\n",
    "        \"baselines\": baselines,\n",
    "        \"threshold\": tuning,\n",
    "        \"r2\": r2_dict\n",
    "    }\n",
    "    with open(os.path.join(OUTDIR, \"metrics_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    write_report(OUTDIR, summary)\n",
    "\n",
    "    log(\"finalizado\")\n",
    "    log(f\"métricas test: {metrics}\")\n",
    "    if tuning: log(f\"umbral óptimo (f1): {tuning}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9889e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
